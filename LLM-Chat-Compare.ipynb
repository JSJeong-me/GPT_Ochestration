{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqY7KJi78DbISu55h00dip",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/GPT_Ochestration/blob/main/LLM-Chat-Compare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "WVzfimdlwIUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgHqkb77wUxN",
        "outputId": "753f444e-de1f-4297-eb27-c2d8966d7f58"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SxlgGEUVn-pc"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "chat_model = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.predict(\"hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "e0EUAxP0wGZ9",
        "outputId": "7ab65283-20dc-4430-ea52-fc0b18c3d1c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\\\\\\\hi!\");\\n    var0.endTextNodeStep();\\n    var0.startFilterExpr();\\n    var0.startNameStep(0, \"com.werken.saxpath.XPathSyntaxException: 10: (0) startAbsoluteLocationPath()\\\\n(1) endAndExpr(true)\\\\n(2) endPathExpr()\\\\n(3) endTextNodeStep()\\\\n(4) startOrExpr()\\\\n\", \"(0) startAbsoluteLocationPath()\\\\n(1) endEqualityExpr(-1)\\\\n(2) endTextNodeStep()\\\\n(3) endAdditiveExpr(1)\\\\n(4) endAndExpr(false)\\\\n(5) endUnaryExpr(0)\\\\n(6) startNameStep(100, \\\\\"org.saxpath.SAXPathException: com.werken.saxpath.XPathSyntaxException: 100: org.saxpath.SAXPathException: \\\\\", \\\\\"com.werken.saxpath.XPathSyntaxException: 1: com.werken.saxpath.XPathSyntax'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.predict(\"hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pfR2U62IwCd0",
        "outputId": "3ec7d3ce-65fd-4dd2-f200-c8d5d695f629"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "\n",
        "llm.predict(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kACM0_H4xNow",
        "outputId": "6b83ff20-460a-4f0a-fbe9-ab87712c3ccc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nHappy Socksy.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.predict(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sRJKduj-xVzO",
        "outputId": "6a267c95-7c5c-41d1-a46c-2be0fdbd06d0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Colorful Soles'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage\n",
        "\n",
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "messages = [HumanMessage(content=text)]"
      ],
      "metadata": {
        "id": "eM0U08eIxsXQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For both these methods, you can also pass in parameters as key word arguments. For example, you could pass in temperature=0 to adjust the temperature that is used from what the object was configured with."
      ],
      "metadata": {
        "id": "WFlvVL4Fx-rR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.predict_messages(messages)\n",
        "# >> Feetful of Fun"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7UKDnR3xvlN",
        "outputId": "47632711-d2f7-479d-d0aa-fd05e09286af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='\\n\\nSocktastic!', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.predict_messages(messages)\n",
        "# >> Socks O'Color"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjH51fzbxc2S",
        "outputId": "1d2df1f4-9f4f-44f9-a74f-4bc1164de6f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Spectrum Socks', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
        "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
        "\n",
        "\n",
        "    def parse(self, text: str):\n",
        "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
        "        return text.strip().split(\", \")\n",
        "\n",
        "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
        "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
        "ONLY return a comma separated list, and nothing more.\"\"\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "chain = LLMChain(\n",
        "    llm=ChatOpenAI(), # openai_api_key=\"sk-\"\n",
        "    prompt=chat_prompt,\n",
        "    output_parser=CommaSeparatedListOutputParser()\n",
        ")\n",
        "chain.run(\"colors\")\n",
        "# >> ['red', 'blue', 'green', 'yellow', 'orange']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glQlfHqIwmNa",
        "outputId": "230f3b40-d96b-4504-9723-92d212efca2a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['red', 'blue', 'green', 'yellow', 'orange']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}