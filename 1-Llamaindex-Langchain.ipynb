{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/GPT_Ochestration/blob/main/1-Llamaindex-Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bf01a75-a01b-472e-bc0c-9fe97658eb46",
      "metadata": {
        "id": "3bf01a75-a01b-472e-bc0c-9fe97658eb46"
      },
      "source": [
        "## LlamaIndex <> Langchain Integrations\n",
        "\n",
        "This demo notebook shows how you can provide integrations between LlamaIndex and Langchain. It provides the following examples:\n",
        "- Using LlamaIndex as a callable tool with a Langchain agent\n",
        "- Using LlamaIndex as a memory module; this allows you to insert arbitrary amounts of conversation history with a Langchain chatbot!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "Ol6-Nyqwk86O"
      },
      "id": "Ol6-Nyqwk86O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index pypdf"
      ],
      "metadata": {
        "id": "Qu8kWA3dlPH7"
      },
      "id": "Qu8kWA3dlPH7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "rCOGQ8iFlYhE"
      },
      "id": "rCOGQ8iFlYhE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Set up the OpenAI API client\n",
        "openai.api_key = \"sk-\"\n"
      ],
      "metadata": {
        "id": "r2uTKgg3leOi"
      },
      "id": "r2uTKgg3leOi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY', 'sk-')"
      ],
      "metadata": {
        "id": "S74nbuhvm_bH"
      },
      "id": "S74nbuhvm_bH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1568569",
      "metadata": {
        "id": "c1568569"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb9177a4-9cb7-4211-b463-121d850b5917",
      "metadata": {
        "id": "bb9177a4-9cb7-4211-b463-121d850b5917"
      },
      "source": [
        "#### Using LlamaIndex as a Callable Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12af1b0e-983f-4fc1-b5b4-2edeb2e8f07e",
      "metadata": {
        "id": "12af1b0e-983f-4fc1-b5b4-2edeb2e8f07e"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import Tool\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "KCwlcBbUmA26"
      },
      "id": "KCwlcBbUmA26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1226132b-2c5f-4073-bfdb-0e36c681c12f",
      "metadata": {
        "id": "1226132b-2c5f-4073-bfdb-0e36c681c12f",
        "outputId": "dceaa938-be31-4640-8106-5e4aa4203c24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents=documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c9b3567-c95c-473d-afc0-516b5f35e197",
      "metadata": {
        "id": "8c9b3567-c95c-473d-afc0-516b5f35e197"
      },
      "outputs": [],
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name=\"LlamaIndex\",\n",
        "        func=lambda q: str(index.as_query_engine().query(q)),\n",
        "        description=\"useful for when you want to answer questions about the author. The input to this tool should be a complete english sentence.\",\n",
        "        return_direct=True,\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "374e9f69-1f75-4a62-afdc-22f748d4bddd",
      "metadata": {
        "id": "374e9f69-1f75-4a62-afdc-22f748d4bddd"
      },
      "outputs": [],
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "llm = ChatOpenAI(temperature=0, openai_api_key='sk-')\n",
        "agent_executor = initialize_agent(\n",
        "    tools, llm, agent=\"conversational-react-description\", memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "579fbc9f-9f13-416c-bde4-7e56fb899727",
      "metadata": {
        "id": "579fbc9f-9f13-416c-bde4-7e56fb899727",
        "outputId": "1d14b3ef-5f9a-4154-94b1-ac25caef6809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Bob! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "agent_executor.run(input=\"hi, i am bob\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9841c8e-f90b-4e40-a2f9-ad1e98bb9eef",
      "metadata": {
        "id": "a9841c8e-f90b-4e40-a2f9-ad1e98bb9eef",
        "outputId": "37fbb9a1-5b79-4cc2-967e-85068162e6fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The author's childhood involved working on writing and programming outside of school. They wrote short stories and tried writing programs on an IBM 1401 computer in 9th grade. They used an early version of Fortran and had to type programs on punch cards. The author also mentioned their fascination with microcomputers and eventually convinced their father to buy a TRS-80 computer. They wrote simple games and a word processor on it. Additionally, the author mentioned their initial plan to study philosophy in college but eventually switched to AI.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "agent_executor.run(input=\"What did the author do growing up?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c02eb88-5a4a-4694-9b77-cd46adc691f5",
      "metadata": {
        "id": "7c02eb88-5a4a-4694-9b77-cd46adc691f5"
      },
      "source": [
        "#### Using LlamaIndex as a memory module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e06a04c1-c5fa-482c-b4d7-9b3fa0f904af",
      "metadata": {
        "id": "e06a04c1-c5fa-482c-b4d7-9b3fa0f904af"
      },
      "outputs": [],
      "source": [
        "# try using SummaryIndex!\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.llms import OpenAIChat\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "from llama_index import SummaryIndex\n",
        "from llama_index.langchain_helpers.memory_wrapper import GPTIndexChatMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00e6694b-25fc-4fbc-8223-9a5605dc641f",
      "metadata": {
        "id": "00e6694b-25fc-4fbc-8223-9a5605dc641f"
      },
      "outputs": [],
      "source": [
        "index = SummaryIndex([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c0b10c-bca4-49f1-9353-646a182050cf",
      "metadata": {
        "id": "25c0b10c-bca4-49f1-9353-646a182050cf"
      },
      "outputs": [],
      "source": [
        "# set Logging to DEBUG for more detailed outputs\n",
        "# NOTE: you can also use a conversational chain\n",
        "\n",
        "memory = GPTIndexChatMemory(\n",
        "    index=index,\n",
        "    memory_key=\"chat_history\",\n",
        "    query_kwargs={\"response_mode\": \"compact\"},\n",
        "    # return_source returns source nodes instead of querying index\n",
        "    return_source=True,\n",
        "    # return_messages returns context in message format\n",
        "    return_messages=True,\n",
        ")\n",
        "# llm = OpenAIChat(temperature=0)\n",
        "llm=OpenAI(temperature=0, openai_api_key='sk-')\n",
        "agent_executor = initialize_agent(\n",
        "    [], llm, agent=\"conversational-react-description\", memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76193275-c47c-426c-b7e4-c54d31fda92d",
      "metadata": {
        "id": "76193275-c47c-426c-b7e4-c54d31fda92d",
        "outputId": "928282ee-d743-42b2-8a7d-52525a4e88bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi Bob, nice to meet you! How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "agent_executor.run(input=\"hi, i am bob\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d426239a-a38b-4ae9-838b-b6fab43970e0",
      "metadata": {
        "id": "d426239a-a38b-4ae9-838b-b6fab43970e0",
        "outputId": "b5158371-41b9-4f67-9029-3a09270e6ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is your name?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# NOTE: the query now calls the SummaryIndex memory module.\n",
        "agent_executor.run(input=\"what's my name?\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}